# SYNAPSE - Technical Specification
## Enterprise-Grade Implementation on $500/Month Budget

**Version:** 1.0  
**Target:** Claude Code AI Assistant  
**Development Timeline:** 12 weeks  
**Quality Standard:** Production/Enterprise-Grade (NOT MVP)

---

## ARCHITECTURE OVERVIEW

### System Architecture Pattern

**Hybrid Serverless + Dedicated Architecture:**
```
┌─────────────────────────────────────────────────────────────┐
│                      User Browsers                          │
│            (Next.js 14 SSR + Client Components)             │
└───────────────────┬─────────────────────────────────────────┘
                    │ HTTPS/WSS
┌───────────────────▼─────────────────────────────────────────┐
│              Cloudflare (CDN + DDoS + Firewall)             │
└───────────────────┬─────────────────────────────────────────┘
                    │
        ┌───────────┴──────────┐
        │                      │
┌───────▼──────────┐    ┌─────▼──────────────┐
│  Vercel Edge     │    │ Railway/DO Servers │
│  (Next.js Apps)  │    │ (WebSocket Server) │
│  - API Routes    │    │ - Socket.io        │
│  - tRPC          │    │ - Redis PubSub     │
│  - SSR Pages     │    │ - Real-time sync   │
└───────┬──────────┘    └─────┬──────────────┘
        │                      │
┌───────▼──────────────────────▼──────────────┐
│         Supabase / Neon PostgreSQL          │
│         - Row-Level Security (RLS)          │
│         - Connection Pooling (PgBouncer)    │
│         - Vector Extension (pgvector)       │
└───────┬─────────────────────────────────────┘
        │
┌───────▼─────────────────────────────────────┐
│           Storage & Services Layer          │
│  ┌──────────────┬──────────────┬─────────┐ │
│  │ Cloudflare R2│  Qdrant      │  Redis  │ │
│  │ (Object      │  (Vectors)   │  (Cache)│ │
│  │  Storage)    │              │         │ │
│  └──────────────┴──────────────┴─────────┘ │
└─────────────────────────────────────────────┘
        │
┌───────▼─────────────────────────────────────┐
│         External Integrations                │
│  GitHub │ Slack │ Linear │ Figma │ Notion  │
│  Zoom   │ Dovetail │ Mixpanel              │
└─────────────────────────────────────────────┘
```

### Technology Stack

**Frontend (Client):**
- **Framework:** Next.js 14.2+ with App Router and React Server Components
- **Language:** TypeScript 5.3+ (strict mode enabled)
- **UI Framework:** Tailwind CSS 3.4+ (JIT mode)
- **Component Library:** shadcn/ui (headless, accessible components)
- **State Management:** 
  - Server state: tRPC + React Query
  - Client state: Zustand (minimal, only for UI state)
  - Real-time state: Yjs CRDT for collaborative features
- **Forms:** React Hook Form + Zod validation
- **Icons:** Lucide React (tree-shakeable)
- **Charts:** Recharts (lightweight, responsive)
- **Rich Text:** TipTap (extensible, ProseMirror-based)

**Backend (Server):**
- **API Framework:** Next.js 14 API Routes + tRPC v10
- **Runtime:** Node.js 20 LTS
- **Auth:** Clerk (free tier → Supabase Auth at 10K users)
- **Database:** PostgreSQL 15+ via Supabase or Neon
- **ORM:** Prisma 5+ (type-safe, auto-migrations)
- **Background Jobs:** Inngest (free tier: 100K executions/month)
- **Real-time:** Socket.io on Railway + Supabase Realtime fallback
- **File Upload:** Presigned URLs to Cloudflare R2 (S3-compatible)
- **Rate Limiting:** Redis with token bucket algorithm
- **Webhooks:** Express.js middleware with signature verification

**AI & ML:**
- **Embeddings:** OpenAI text-embedding-3-small ($0.01/1M tokens)
- **LLM Router:**
  - Simple queries: GPT-3.5-turbo ($0.50/$1.50 per 1M tokens)
  - Complex queries: Claude Sonnet 4 ($3/$15 per 1M tokens)
- **Vector Database:** 
  - Start: pgvector extension in PostgreSQL
  - Scale: Self-hosted Qdrant on $20/month VPS
- **Prompt Caching:** Anthropic prompt caching (90% cost reduction)
- **Query Classification:** Heuristic rules (length, keywords, complexity)

**Infrastructure:**
- **Hosting:** Vercel (free tier) + Railway or DigitalOcean ($90-180/month)
- **Database:** Supabase (free tier → Pro $25/month) or Neon (free tier → Pro $69/month)
- **Object Storage:** Cloudflare R2 ($30/month for 2TB)
- **Cache:** Upstash Redis (free tier → $10/month)
- **Vector DB:** Qdrant Cloud free tier or self-hosted ($20/month)
- **CDN:** Cloudflare (free tier with unlimited bandwidth)
- **Monitoring:** Axiom (free tier), Better Stack (free tier), Rollbar (free tier)
- **Email:** Resend (free tier: 3,000 emails/month)

**Total Infrastructure Cost Projection:**
```
Month 0 (Launch): $0-30/month (all free tiers)
Month 1 (50 users): $55-115/month
Month 3 (200 users): $150-275/month
Month 6 (500 users): $255-450/month
```

---

## DATABASE SCHEMA

### Core Tables

**organizations**
```sql
CREATE TABLE organizations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  slug VARCHAR(100) UNIQUE NOT NULL,
  domain VARCHAR(255), -- For email-based auto-join
  
  -- Subscription
  plan_tier VARCHAR(50) NOT NULL DEFAULT 'free', 
    -- Values: free, starter, professional, enterprise
  subscription_status VARCHAR(50) DEFAULT 'active',
  stripe_customer_id VARCHAR(255),
  stripe_subscription_id VARCHAR(255),
  billing_email VARCHAR(255),
  
  -- Storage
  storage_limit_gb INT NOT NULL DEFAULT 25, -- 25GB for free tier
  storage_used_bytes BIGINT NOT NULL DEFAULT 0,
  
  -- Settings
  settings JSONB DEFAULT '{}', -- {onboarding_completed, feature_flags, etc}
  
  -- Metadata
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  deleted_at TIMESTAMPTZ -- Soft delete
);

CREATE INDEX idx_organizations_slug ON organizations(slug);
CREATE INDEX idx_organizations_stripe_customer ON organizations(stripe_customer_id);
```

**users**
```sql
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Auth (managed by Clerk/Supabase)
  auth_provider_id VARCHAR(255) UNIQUE NOT NULL, -- Clerk user ID or Supabase auth.users.id
  email VARCHAR(255) NOT NULL,
  full_name VARCHAR(255),
  avatar_url TEXT,
  
  -- Permissions
  role VARCHAR(50) NOT NULL DEFAULT 'member',
    -- Values: owner, admin, member, viewer
  
  -- Storage (per-user allocation)
  storage_quota_gb INT NOT NULL DEFAULT 100,
  storage_used_bytes BIGINT NOT NULL DEFAULT 0,
  
  -- Preferences
  preferences JSONB DEFAULT '{}', 
    -- {notification_settings, theme, keyboard_shortcuts, etc}
  
  -- Metadata
  last_active_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  deleted_at TIMESTAMPTZ
);

CREATE INDEX idx_users_org ON users(organization_id);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_auth_provider ON users(auth_provider_id);

-- Row-Level Security
ALTER TABLE users ENABLE ROW LEVEL SECURITY;

CREATE POLICY users_select ON users 
  FOR SELECT 
  USING (organization_id = current_setting('app.current_tenant')::UUID);

CREATE POLICY users_insert ON users 
  FOR INSERT 
  WITH CHECK (organization_id = current_setting('app.current_tenant')::UUID);
```

**golden_threads**
```sql
CREATE TABLE golden_threads (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Core Properties
  title VARCHAR(500) NOT NULL,
  description TEXT,
  status VARCHAR(50) NOT NULL DEFAULT 'planning',
    -- Values: planning, in_progress, review, completed, archived
  
  -- Organization
  tags TEXT[] DEFAULT '{}', -- Array of tag strings
  project_id UUID REFERENCES projects(id), -- Optional project grouping
  
  -- Access Control
  visibility VARCHAR(50) NOT NULL DEFAULT 'team',
    -- Values: private, team, organization, public
  created_by UUID NOT NULL REFERENCES users(id),
  
  -- Computed Fields (updated by triggers)
  connected_items_count INT NOT NULL DEFAULT 0,
  last_activity_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  
  -- AI Features
  embedding VECTOR(1536), -- OpenAI text-embedding-3-small dimensions
  
  -- Metadata
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  deleted_at TIMESTAMPTZ,
  
  -- Full-text search
  search_vector TSVECTOR GENERATED ALWAYS AS (
    setweight(to_tsvector('english', COALESCE(title, '')), 'A') ||
    setweight(to_tsvector('english', COALESCE(description, '')), 'B') ||
    setweight(to_tsvector('english', COALESCE(array_to_string(tags, ' '), '')), 'C')
  ) STORED
);

CREATE INDEX idx_threads_org ON golden_threads(organization_id);
CREATE INDEX idx_threads_status ON golden_threads(status);
CREATE INDEX idx_threads_created_by ON golden_threads(created_by);
CREATE INDEX idx_threads_last_activity ON golden_threads(last_activity_at DESC);
CREATE INDEX idx_threads_search ON golden_threads USING GIN(search_vector);
CREATE INDEX idx_threads_embedding ON golden_threads USING ivfflat(embedding vector_cosine_ops);

-- Row-Level Security
ALTER TABLE golden_threads ENABLE ROW LEVEL SECURITY;

CREATE POLICY threads_select ON golden_threads 
  FOR SELECT 
  USING (
    organization_id = current_setting('app.current_tenant')::UUID
    AND (
      visibility = 'organization'
      OR (visibility = 'team' AND created_by IN (
        SELECT id FROM users WHERE organization_id = current_setting('app.current_tenant')::UUID
      ))
      OR (visibility = 'private' AND created_by = current_setting('app.current_user')::UUID)
    )
  );
```

**thread_collaborators**
```sql
CREATE TABLE thread_collaborators (
  thread_id UUID NOT NULL REFERENCES golden_threads(id) ON DELETE CASCADE,
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  
  role VARCHAR(50) NOT NULL DEFAULT 'viewer',
    -- Values: owner, editor, commenter, viewer
  
  -- Notifications
  subscribed_to_updates BOOLEAN NOT NULL DEFAULT TRUE,
  last_viewed_at TIMESTAMPTZ,
  
  -- Metadata
  added_by UUID NOT NULL REFERENCES users(id),
  added_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  
  PRIMARY KEY (thread_id, user_id)
);

CREATE INDEX idx_collaborators_user ON thread_collaborators(user_id);
CREATE INDEX idx_collaborators_thread ON thread_collaborators(thread_id);
```

**connected_items**
```sql
CREATE TABLE connected_items (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  thread_id UUID NOT NULL REFERENCES golden_threads(id) ON DELETE CASCADE,
  
  -- Integration Source
  integration_type VARCHAR(50) NOT NULL,
    -- Values: figma, linear, github, slack, notion, zoom, dovetail, mixpanel
  external_id VARCHAR(500) NOT NULL, -- ID in external system
  external_url TEXT, -- Direct link to item
  
  -- Item Data (cached from external system)
  item_type VARCHAR(100), -- e.g., "figma_file", "linear_issue", "github_pr"
  title VARCHAR(1000),
  description TEXT,
  metadata JSONB DEFAULT '{}', 
    -- Flexible storage for integration-specific data
  thumbnail_url TEXT,
  
  -- Status
  sync_status VARCHAR(50) DEFAULT 'synced',
    -- Values: synced, pending, error, deleted_at_source
  last_synced_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  
  -- AI Features
  embedding VECTOR(1536),
  
  -- Metadata
  created_by UUID NOT NULL REFERENCES users(id),
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  deleted_at TIMESTAMPTZ,
  
  -- Full-text search
  search_vector TSVECTOR GENERATED ALWAYS AS (
    setweight(to_tsvector('english', COALESCE(title, '')), 'A') ||
    setweight(to_tsvector('english', COALESCE(description, '')), 'B')
  ) STORED,
  
  -- Ensure uniqueness per thread
  UNIQUE(thread_id, integration_type, external_id)
);

CREATE INDEX idx_items_thread ON connected_items(thread_id);
CREATE INDEX idx_items_integration ON connected_items(integration_type);
CREATE INDEX idx_items_external_id ON connected_items(external_id);
CREATE INDEX idx_items_search ON connected_items USING GIN(search_vector);
CREATE INDEX idx_items_embedding ON connected_items USING ivfflat(embedding vector_cosine_ops);

-- Row-Level Security
ALTER TABLE connected_items ENABLE ROW LEVEL SECURITY;

CREATE POLICY items_select ON connected_items 
  FOR SELECT 
  USING (organization_id = current_setting('app.current_tenant')::UUID);
```

**integrations**
```sql
CREATE TABLE integrations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Integration Details
  integration_type VARCHAR(50) NOT NULL,
    -- Values: figma, linear, github, slack, notion, zoom, dovetail, mixpanel
  
  -- OAuth Tokens (ENCRYPTED)
  encrypted_access_token TEXT, -- pgcrypto encrypted with tenant key
  encrypted_refresh_token TEXT,
  token_expires_at TIMESTAMPTZ,
  
  -- Integration Metadata
  external_user_id VARCHAR(255), -- User ID in external system
  external_workspace_id VARCHAR(255), -- Workspace/org ID in external system
  scopes TEXT[], -- OAuth scopes granted
  
  -- Status
  status VARCHAR(50) NOT NULL DEFAULT 'active',
    -- Values: active, expired, revoked, error
  last_sync_at TIMESTAMPTZ,
  error_message TEXT, -- Last error if status = error
  
  -- Rate Limiting
  rate_limit_remaining INT,
  rate_limit_reset_at TIMESTAMPTZ,
  
  -- Metadata
  connected_by UUID NOT NULL REFERENCES users(id),
  connected_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  deleted_at TIMESTAMPTZ,
  
  -- One integration per type per org (can reconnect to update tokens)
  UNIQUE(organization_id, integration_type)
);

CREATE INDEX idx_integrations_org ON integrations(organization_id);
CREATE INDEX idx_integrations_type ON integrations(integration_type);
CREATE INDEX idx_integrations_status ON integrations(status);

-- Row-Level Security
ALTER TABLE integrations ENABLE ROW LEVEL SECURITY;

CREATE POLICY integrations_select ON integrations 
  FOR SELECT 
  USING (organization_id = current_setting('app.current_tenant')::UUID);
```

**automations**
```sql
CREATE TABLE automations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Automation Definition
  name VARCHAR(255) NOT NULL,
  description TEXT,
  template_id VARCHAR(100), -- Reference to pre-built template if used
  
  -- Workflow Definition (JSON structure)
  trigger JSONB NOT NULL, 
    -- {type: "figma_file_updated", conditions: {...}}
  conditions JSONB DEFAULT '[]',
    -- [{field: "status", operator: "equals", value: "ready"}]
  actions JSONB NOT NULL,
    -- [{type: "create_linear_issue", params: {...}}, ...]
  
  -- Status
  is_active BOOLEAN NOT NULL DEFAULT TRUE,
  last_triggered_at TIMESTAMPTZ,
  execution_count INT NOT NULL DEFAULT 0,
  error_count INT NOT NULL DEFAULT 0,
  
  -- Metadata
  created_by UUID NOT NULL REFERENCES users(id),
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  deleted_at TIMESTAMPTZ
);

CREATE INDEX idx_automations_org ON automations(organization_id);
CREATE INDEX idx_automations_active ON automations(is_active);
CREATE INDEX idx_automations_template ON automations(template_id);

-- Row-Level Security
ALTER TABLE automations ENABLE ROW LEVEL SECURITY;

CREATE POLICY automations_select ON automations 
  FOR SELECT 
  USING (organization_id = current_setting('app.current_tenant')::UUID);
```

**automation_runs**
```sql
CREATE TABLE automation_runs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  automation_id UUID NOT NULL REFERENCES automations(id) ON DELETE CASCADE,
  
  -- Execution Details
  status VARCHAR(50) NOT NULL,
    -- Values: pending, running, success, failed, cancelled
  trigger_event JSONB NOT NULL, -- The event that triggered this run
  
  -- Results
  actions_executed JSONB DEFAULT '[]',
    -- [{action_type, status, result, error}, ...]
  error_message TEXT,
  
  -- Timing
  started_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  completed_at TIMESTAMPTZ,
  duration_ms INT, -- Computed: completed_at - started_at
  
  -- Metadata
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_runs_automation ON automation_runs(automation_id);
CREATE INDEX idx_runs_status ON automation_runs(status);
CREATE INDEX idx_runs_created_at ON automation_runs(created_at DESC);

-- Partition by month for performance (optional, implement after 100K runs)
-- CREATE TABLE automation_runs_2025_10 PARTITION OF automation_runs
--   FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');
```

**comments**
```sql
CREATE TABLE comments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Comment Target
  thread_id UUID NOT NULL REFERENCES golden_threads(id) ON DELETE CASCADE,
  item_id UUID REFERENCES connected_items(id) ON DELETE CASCADE, 
    -- NULL if thread-level comment
  parent_comment_id UUID REFERENCES comments(id) ON DELETE CASCADE, 
    -- NULL if top-level comment
  
  -- Content
  content TEXT NOT NULL,
  content_html TEXT, -- Rendered HTML with mentions highlighted
  mentions UUID[] DEFAULT '{}', -- Array of mentioned user IDs
  
  -- Status
  is_edited BOOLEAN NOT NULL DEFAULT FALSE,
  edited_at TIMESTAMPTZ,
  is_deleted BOOLEAN NOT NULL DEFAULT FALSE, -- Soft delete to preserve thread
  
  -- Reactions
  reaction_counts JSONB DEFAULT '{}', 
    -- {👍: 5, ❤️: 3, ...} updated via trigger
  
  -- Metadata
  created_by UUID NOT NULL REFERENCES users(id),
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_comments_thread ON comments(thread_id);
CREATE INDEX idx_comments_item ON comments(item_id);
CREATE INDEX idx_comments_parent ON comments(parent_comment_id);
CREATE INDEX idx_comments_created_by ON comments(created_by);
CREATE INDEX idx_comments_created_at ON comments(created_at DESC);

-- Row-Level Security
ALTER TABLE comments ENABLE ROW LEVEL SECURITY;

CREATE POLICY comments_select ON comments 
  FOR SELECT 
  USING (organization_id = current_setting('app.current_tenant')::UUID);
```

**activity_feed**
```sql
CREATE TABLE activity_feed (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Activity Details
  actor_id UUID NOT NULL REFERENCES users(id), -- Who did the action
  action_type VARCHAR(100) NOT NULL,
    -- Values: thread_created, item_added, comment_posted, 
    --         status_changed, automation_ran, etc.
  
  -- Activity Target
  thread_id UUID REFERENCES golden_threads(id) ON DELETE CASCADE,
  item_id UUID REFERENCES connected_items(id) ON DELETE CASCADE,
  comment_id UUID REFERENCES comments(id) ON DELETE CASCADE,
  
  -- Activity Data
  metadata JSONB DEFAULT '{}',
    -- Flexible storage for action-specific data
  
  -- Metadata
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_activity_org ON activity_feed(organization_id);
CREATE INDEX idx_activity_thread ON activity_feed(thread_id);
CREATE INDEX idx_activity_created_at ON activity_feed(created_at DESC);

-- Partition by month for performance
-- Similar to automation_runs, partition after significant data
```

**webhooks**
```sql
CREATE TABLE webhooks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  -- Webhook Details
  integration_type VARCHAR(50) NOT NULL,
  event_type VARCHAR(100) NOT NULL, -- e.g., "file.updated", "issue.created"
  
  -- Payload
  payload JSONB NOT NULL,
  signature VARCHAR(500), -- Webhook signature for verification
  
  -- Processing Status
  status VARCHAR(50) NOT NULL DEFAULT 'pending',
    -- Values: pending, processing, processed, failed, discarded
  processed_at TIMESTAMPTZ,
  error_message TEXT,
  retry_count INT NOT NULL DEFAULT 0,
  
  -- Metadata
  received_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_webhooks_status ON webhooks(status);
CREATE INDEX idx_webhooks_integration ON webhooks(integration_type);
CREATE INDEX idx_webhooks_received_at ON webhooks(received_at DESC);

-- Auto-delete processed webhooks after 30 days (retention policy)
-- Implement via cron job: DELETE FROM webhooks WHERE status = 'processed' AND received_at < NOW() - INTERVAL '30 days';
```

### Supporting Tables

**projects** (Optional grouping)
```sql
CREATE TABLE projects (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  name VARCHAR(255) NOT NULL,
  description TEXT,
  color VARCHAR(7), -- Hex color for visual grouping
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

**notifications**
```sql
CREATE TABLE notifications (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  
  notification_type VARCHAR(100) NOT NULL,
    -- Values: mention, comment_reply, thread_update, automation_error, etc.
  title VARCHAR(500) NOT NULL,
  message TEXT,
  link_url TEXT, -- Where to navigate when clicked
  
  is_read BOOLEAN NOT NULL DEFAULT FALSE,
  read_at TIMESTAMPTZ,
  
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_notifications_user ON notifications(user_id);
CREATE INDEX idx_notifications_unread ON notifications(user_id, is_read);
CREATE INDEX idx_notifications_created_at ON notifications(created_at DESC);
```

**api_keys** (For programmatic access)
```sql
CREATE TABLE api_keys (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  
  name VARCHAR(255) NOT NULL,
  key_prefix VARCHAR(10) NOT NULL, -- e.g., "sk_prod_"
  key_hash VARCHAR(255) NOT NULL, -- bcrypt hash of full key
  
  scopes TEXT[] DEFAULT '{}', -- Permissions granted
  last_used_at TIMESTAMPTZ,
  
  created_by UUID NOT NULL REFERENCES users(id),
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  expires_at TIMESTAMPTZ -- NULL = never expires
);

CREATE INDEX idx_api_keys_org ON api_keys(organization_id);
CREATE INDEX idx_api_keys_prefix ON api_keys(key_prefix);
```

---

## API DESIGN

### tRPC Router Structure

**File: `/src/server/api/root.ts`**
```typescript
import { createTRPCRouter } from './trpc';
import { authRouter } from './routers/auth';
import { organizationsRouter } from './routers/organizations';
import { threadsRouter } from './routers/threads';
import { itemsRouter } from './routers/items';
import { integrationsRouter } from './routers/integrations';
import { automationsRouter } from './routers/automations';
import { commentsRouter } from './routers/comments';
import { searchRouter } from './routers/search';
import { analyticsRouter } from './routers/analytics';

export const appRouter = createTRPCRouter({
  auth: authRouter,
  organizations: organizationsRouter,
  threads: threadsRouter,
  items: itemsRouter,
  integrations: integrationsRouter,
  automations: automationsRouter,
  comments: commentsRouter,
  search: searchRouter,
  analytics: analyticsRouter,
});

export type AppRouter = typeof appRouter;
```

### Key tRPC Procedures

**Threads Router**
```typescript
// File: /src/server/api/routers/threads.ts

import { z } from 'zod';
import { createTRPCRouter, protectedProcedure } from '../trpc';

export const threadsRouter = createTRPCRouter({
  
  // List threads with pagination and filters
  list: protectedProcedure
    .input(z.object({
      cursor: z.string().uuid().optional(),
      limit: z.number().min(1).max(100).default(20),
      status: z.enum(['planning', 'in_progress', 'review', 'completed', 'archived']).optional(),
      tags: z.array(z.string()).optional(),
      search: z.string().optional(),
    }))
    .query(async ({ ctx, input }) => {
      const threads = await ctx.prisma.goldenThread.findMany({
        where: {
          organizationId: ctx.session.organizationId,
          status: input.status,
          tags: input.tags ? { hasSome: input.tags } : undefined,
          OR: input.search ? [
            { title: { contains: input.search, mode: 'insensitive' } },
            { description: { contains: input.search, mode: 'insensitive' } },
          ] : undefined,
          deletedAt: null,
        },
        take: input.limit + 1,
        cursor: input.cursor ? { id: input.cursor } : undefined,
        orderBy: { lastActivityAt: 'desc' },
        include: {
          createdBy: { select: { id: true, fullName: true, avatarUrl: true } },
          _count: { select: { connectedItems: true, comments: true } },
        },
      });
      
      let nextCursor: string | undefined = undefined;
      if (threads.length > input.limit) {
        const nextItem = threads.pop();
        nextCursor = nextItem!.id;
      }
      
      return { threads, nextCursor };
    }),

  // Get single thread with full details
  getById: protectedProcedure
    .input(z.object({ id: z.string().uuid() }))
    .query(async ({ ctx, input }) => {
      const thread = await ctx.prisma.goldenThread.findFirst({
        where: {
          id: input.id,
          organizationId: ctx.session.organizationId,
          deletedAt: null,
        },
        include: {
          createdBy: true,
          collaborators: {
            include: { user: true },
          },
          connectedItems: {
            where: { deletedAt: null },
            orderBy: { createdAt: 'asc' },
            include: {
              createdBy: { select: { id: true, fullName: true, avatarUrl: true } },
            },
          },
          comments: {
            where: { isDeleted: false, parentCommentId: null },
            orderBy: { createdAt: 'desc' },
            take: 20,
            include: {
              createdBy: true,
              _count: { select: { replies: true } },
            },
          },
        },
      });
      
      if (!thread) {
        throw new TRPCError({ code: 'NOT_FOUND', message: 'Thread not found' });
      }
      
      return thread;
    }),

  // Create new thread
  create: protectedProcedure
    .input(z.object({
      title: z.string().min(1).max(500),
      description: z.string().max(10000).optional(),
      status: z.enum(['planning', 'in_progress', 'review', 'completed', 'archived']).default('planning'),
      tags: z.array(z.string()).default([]),
      collaboratorIds: z.array(z.string().uuid()).default([]),
    }))
    .mutation(async ({ ctx, input }) => {
      // Create thread
      const thread = await ctx.prisma.goldenThread.create({
        data: {
          organizationId: ctx.session.organizationId,
          title: input.title,
          description: input.description,
          status: input.status,
          tags: input.tags,
          createdBy: ctx.session.userId,
        },
      });
      
      // Add creator as owner collaborator
      await ctx.prisma.threadCollaborator.create({
        data: {
          threadId: thread.id,
          userId: ctx.session.userId,
          role: 'owner',
          addedBy: ctx.session.userId,
        },
      });
      
      // Add additional collaborators
      if (input.collaboratorIds.length > 0) {
        await ctx.prisma.threadCollaborator.createMany({
          data: input.collaboratorIds.map(userId => ({
            threadId: thread.id,
            userId,
            role: 'editor',
            addedBy: ctx.session.userId,
          })),
        });
      }
      
      // Generate embedding asynchronously
      await ctx.inngest.send({
        name: 'thread.embedding.generate',
        data: { threadId: thread.id },
      });
      
      return thread;
    }),

  // Update thread
  update: protectedProcedure
    .input(z.object({
      id: z.string().uuid(),
      title: z.string().min(1).max(500).optional(),
      description: z.string().max(10000).optional(),
      status: z.enum(['planning', 'in_progress', 'review', 'completed', 'archived']).optional(),
      tags: z.array(z.string()).optional(),
    }))
    .mutation(async ({ ctx, input }) => {
      // Check permissions
      const isCollaborator = await ctx.prisma.threadCollaborator.findFirst({
        where: {
          threadId: input.id,
          userId: ctx.session.userId,
          role: { in: ['owner', 'editor'] },
        },
      });
      
      if (!isCollaborator) {
        throw new TRPCError({ code: 'FORBIDDEN', message: 'No permission to edit this thread' });
      }
      
      const thread = await ctx.prisma.goldenThread.update({
        where: { id: input.id },
        data: {
          title: input.title,
          description: input.description,
          status: input.status,
          tags: input.tags,
          updatedAt: new Date(),
        },
      });
      
      // Re-generate embedding if title/description changed
      if (input.title || input.description) {
        await ctx.inngest.send({
          name: 'thread.embedding.regenerate',
          data: { threadId: thread.id },
        });
      }
      
      return thread;
    }),

  // Delete thread (soft delete)
  delete: protectedProcedure
    .input(z.object({ id: z.string().uuid() }))
    .mutation(async ({ ctx, input }) => {
      // Check ownership
      const thread = await ctx.prisma.goldenThread.findFirst({
        where: {
          id: input.id,
          organizationId: ctx.session.organizationId,
        },
      });
      
      if (!thread) {
        throw new TRPCError({ code: 'NOT_FOUND' });
      }
      
      if (thread.createdBy !== ctx.session.userId) {
        throw new TRPCError({ code: 'FORBIDDEN', message: 'Only thread creator can delete' });
      }
      
      await ctx.prisma.goldenThread.update({
        where: { id: input.id },
        data: { deletedAt: new Date() },
      });
      
      return { success: true };
    }),
});
```

**Search Router with Hybrid Search**
```typescript
// File: /src/server/api/routers/search.ts

import { z } from 'zod';
import { createTRPCRouter, protectedProcedure } from '../trpc';

export const searchRouter = createTRPCRouter({
  
  // Unified search across threads and items
  unified: protectedProcedure
    .input(z.object({
      query: z.string().min(1).max(500),
      filters: z.object({
        integration: z.enum(['figma', 'linear', 'github', 'slack', 'notion', 'zoom', 'dovetail', 'mixpanel']).optional(),
        dateRange: z.object({
          from: z.date().optional(),
          to: z.date().optional(),
        }).optional(),
        threadId: z.string().uuid().optional(),
      }).optional(),
      limit: z.number().min(1).max(50).default(20),
    }))
    .query(async ({ ctx, input }) => {
      // Step 1: Full-text search (BM25) for keyword matching
      const ftsQuery = await ctx.prisma.$queryRaw`
        SELECT 
          'thread' as type,
          id,
          title,
          description,
          ts_rank(search_vector, websearch_to_tsquery('english', ${input.query})) as rank
        FROM golden_threads
        WHERE 
          organization_id = ${ctx.session.organizationId}
          AND deleted_at IS NULL
          AND search_vector @@ websearch_to_tsquery('english', ${input.query})
        ORDER BY rank DESC
        LIMIT 20
        
        UNION ALL
        
        SELECT 
          'item' as type,
          id,
          title,
          description,
          ts_rank(search_vector, websearch_to_tsquery('english', ${input.query})) as rank
        FROM connected_items
        WHERE 
          organization_id = ${ctx.session.organizationId}
          AND deleted_at IS NULL
          AND search_vector @@ websearch_to_tsquery('english', ${input.query})
          ${input.filters?.integration ? Prisma.sql`AND integration_type = ${input.filters.integration}` : Prisma.empty}
          ${input.filters?.threadId ? Prisma.sql`AND thread_id = ${input.filters.threadId}` : Prisma.empty}
        ORDER BY rank DESC
        LIMIT 20
      `;
      
      // Step 2: Generate embedding for semantic search
      const embedding = await ctx.openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: input.query,
      });
      
      const queryVector = embedding.data[0].embedding;
      
      // Step 3: Vector similarity search (semantic)
      const vectorQuery = await ctx.prisma.$queryRaw`
        SELECT 
          'thread' as type,
          id,
          title,
          description,
          1 - (embedding <=> ${queryVector}::vector) as similarity
        FROM golden_threads
        WHERE 
          organization_id = ${ctx.session.organizationId}
          AND deleted_at IS NULL
          AND embedding IS NOT NULL
        ORDER BY embedding <=> ${queryVector}::vector
        LIMIT 20
        
        UNION ALL
        
        SELECT 
          'item' as type,
          id,
          title,
          description,
          1 - (embedding <=> ${queryVector}::vector) as similarity
        FROM connected_items
        WHERE 
          organization_id = ${ctx.session.organizationId}
          AND deleted_at IS NULL
          AND embedding IS NOT NULL
          ${input.filters?.integration ? Prisma.sql`AND integration_type = ${input.filters.integration}` : Prisma.empty}
          ${input.filters?.threadId ? Prisma.sql`AND thread_id = ${input.filters.threadId}` : Prisma.empty}
        ORDER BY embedding <=> ${queryVector}::vector
        LIMIT 20
      `;
      
      // Step 4: Merge results with weighted score
      // FTS rank (0-1) weighted 40%, Vector similarity (0-1) weighted 60%
      const mergedResults = mergeSearchResults(ftsQuery, vectorQuery, 0.4, 0.6);
      
      // Step 5: Hydrate with full objects
      const threads = await ctx.prisma.goldenThread.findMany({
        where: {
          id: { in: mergedResults.filter(r => r.type === 'thread').map(r => r.id) },
        },
        include: {
          createdBy: { select: { fullName: true, avatarUrl: true } },
          _count: { select: { connectedItems: true } },
        },
      });
      
      const items = await ctx.prisma.connectedItem.findMany({
        where: {
          id: { in: mergedResults.filter(r => r.type === 'item').map(r => r.id) },
        },
        include: {
          thread: { select: { id: true, title: true } },
          createdBy: { select: { fullName: true, avatarUrl: true } },
        },
      });
      
      return {
        threads: threads.slice(0, input.limit / 2),
        items: items.slice(0, input.limit / 2),
      };
    }),
});

// Utility function to merge search results
function mergeSearchResults(
  ftsResults: Array<{ id: string; type: string; rank: number }>,
  vectorResults: Array<{ id: string; type: string; similarity: number }>,
  ftsWeight: number,
  vectorWeight: number
) {
  const scoreMap = new Map<string, { id: string; type: string; score: number }>();
  
  // Add FTS scores
  ftsResults.forEach(result => {
    scoreMap.set(result.id, {
      id: result.id,
      type: result.type,
      score: result.rank * ftsWeight,
    });
  });
  
  // Add vector scores
  vectorResults.forEach(result => {
    const existing = scoreMap.get(result.id);
    if (existing) {
      existing.score += result.similarity * vectorWeight;
    } else {
      scoreMap.set(result.id, {
        id: result.id,
        type: result.type,
        score: result.similarity * vectorWeight,
      });
    }
  });
  
  // Sort by combined score
  return Array.from(scoreMap.values()).sort((a, b) => b.score - a.score);
}
```

### REST Endpoints for Webhooks

**File: `/src/pages/api/webhooks/[integration].ts`**
```typescript
import type { NextApiRequest, NextApiResponse } from 'next';
import { verifyWebhookSignature } from '@/lib/webhooks';
import { prisma } from '@/lib/prisma';
import { inngest } from '@/lib/inngest';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }
  
  const integration = req.query.integration as string;
  
  // Step 1: Verify webhook signature (CRITICAL for security)
  const signature = req.headers['x-hub-signature-256'] || 
                   req.headers['x-slack-signature'] || 
                   req.headers['figma-signature'] as string;
  
  const isValid = await verifyWebhookSignature(
    integration,
    req.body,
    signature
  );
  
  if (!isValid) {
    console.error(`Invalid webhook signature for ${integration}`);
    return res.status(401).json({ error: 'Invalid signature' });
  }
  
  // Step 2: Store webhook payload immediately (idempotency + audit trail)
  const webhook = await prisma.webhook.create({
    data: {
      organizationId: req.body.organization_id || req.body.team_id, // Extract from payload
      integrationType: integration,
      eventType: req.body.event || req.body.type,
      payload: req.body,
      signature,
      status: 'pending',
    },
  });
  
  // Step 3: Respond with 200 OK within 100ms (important for webhook reliability)
  res.status(200).json({ received: true, id: webhook.id });
  
  // Step 4: Process asynchronously via Inngest (after response sent)
  await inngest.send({
    name: `webhook.${integration}.received`,
    data: {
      webhookId: webhook.id,
      eventType: req.body.event || req.body.type,
      payload: req.body,
    },
  });
}

// Disable body parsing to get raw body for signature verification
export const config = {
  api: {
    bodyParser: false,
  },
};
```

---

## OAUTH INTEGRATION IMPLEMENTATION

### OAuth Flow Architecture

```
User clicks "Connect Figma"
    ↓
Frontend: Redirect to /api/oauth/figma/authorize
    ↓
Backend: Generate state token, redirect to Figma OAuth URL
    ↓
User approves on Figma
    ↓
Figma redirects to /api/oauth/figma/callback?code=xxx&state=yyy
    ↓
Backend: Exchange code for access_token + refresh_token
    ↓
Backend: Store encrypted tokens in database
    ↓
Backend: Redirect to dashboard with success message
```

### Generic OAuth Handler

**File: `/src/lib/oauth/base.ts`**
```typescript
import { randomBytes } from 'crypto';
import { prisma } from '@/lib/prisma';
import { encrypt, decrypt } from '@/lib/encryption';

export interface OAuthConfig {
  authorizationUrl: string;
  tokenUrl: string;
  clientId: string;
  clientSecret: string;
  scopes: string[];
  redirectUri: string;
}

export abstract class BaseOAuthHandler {
  protected config: OAuthConfig;
  
  constructor(config: OAuthConfig) {
    this.config = config;
  }
  
  // Step 1: Generate authorization URL
  getAuthorizationUrl(organizationId: string): string {
    const state = this.generateState(organizationId);
    
    const params = new URLSearchParams({
      client_id: this.config.clientId,
      redirect_uri: this.config.redirectUri,
      scope: this.config.scopes.join(' '),
      state,
      response_type: 'code',
    });
    
    return `${this.config.authorizationUrl}?${params}`;
  }
  
  // Step 2: Exchange authorization code for tokens
  async exchangeCodeForTokens(code: string): Promise<{
    accessToken: string;
    refreshToken?: string;
    expiresIn?: number;
  }> {
    const response = await fetch(this.config.tokenUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
      },
      body: new URLSearchParams({
        grant_type: 'authorization_code',
        code,
        client_id: this.config.clientId,
        client_secret: this.config.clientSecret,
        redirect_uri: this.config.redirectUri,
      }),
    });
    
    if (!response.ok) {
      throw new Error(`Token exchange failed: ${response.statusText}`);
    }
    
    const data = await response.json();
    
    return {
      accessToken: data.access_token,
      refreshToken: data.refresh_token,
      expiresIn: data.expires_in,
    };
  }
  
  // Step 3: Refresh access token using refresh token
  async refreshAccessToken(refreshToken: string): Promise<{
    accessToken: string;
    refreshToken?: string;
    expiresIn?: number;
  }> {
    const response = await fetch(this.config.tokenUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
      },
      body: new URLSearchParams({
        grant_type: 'refresh_token',
        refresh_token: refreshToken,
        client_id: this.config.clientId,
        client_secret: this.config.clientSecret,
      }),
    });
    
    if (!response.ok) {
      throw new Error(`Token refresh failed: ${response.statusText}`);
    }
    
    const data = await response.json();
    
    return {
      accessToken: data.access_token,
      refreshToken: data.refresh_token || refreshToken, // Some APIs don't return new refresh token
      expiresIn: data.expires_in,
    };
  }
  
  // Step 4: Store tokens securely
  async storeTokens(
    organizationId: string,
    integrationType: string,
    tokens: {
      accessToken: string;
      refreshToken?: string;
      expiresIn?: number;
    },
    metadata: {
      externalUserId?: string;
      externalWorkspaceId?: string;
      scopes?: string[];
    }
  ): Promise<void> {
    const encryptedAccessToken = await encrypt(tokens.accessToken, organizationId);
    const encryptedRefreshToken = tokens.refreshToken 
      ? await encrypt(tokens.refreshToken, organizationId)
      : null;
    
    const expiresAt = tokens.expiresIn
      ? new Date(Date.now() + tokens.expiresIn * 1000)
      : null;
    
    await prisma.integration.upsert({
      where: {
        organizationId_integrationType: {
          organizationId,
          integrationType,
        },
      },
      update: {
        encryptedAccessToken,
        encryptedRefreshToken,
        tokenExpiresAt: expiresAt,
        externalUserId: metadata.externalUserId,
        externalWorkspaceId: metadata.externalWorkspaceId,
        scopes: metadata.scopes,
        status: 'active',
        updatedAt: new Date(),
      },
      create: {
        organizationId,
        integrationType,
        encryptedAccessToken,
        encryptedRefreshToken,
        tokenExpiresAt: expiresAt,
        externalUserId: metadata.externalUserId,
        externalWorkspaceId: metadata.externalWorkspaceId,
        scopes: metadata.scopes,
        status: 'active',
        connectedBy: metadata.connectedBy,
      },
    });
  }
  
  // Helper: Generate cryptographically secure state token
  private generateState(organizationId: string): string {
    const state = {
      organizationId,
      nonce: randomBytes(16).toString('hex'),
      timestamp: Date.now(),
    };
    
    return Buffer.from(JSON.stringify(state)).toString('base64');
  }
  
  // Helper: Verify and parse state token
  protected parseState(state: string): { organizationId: string; nonce: string; timestamp: number } {
    try {
      const decoded = Buffer.from(state, 'base64').toString('utf-8');
      const parsed = JSON.parse(decoded);
      
      // Verify state is recent (within 10 minutes)
      if (Date.now() - parsed.timestamp > 10 * 60 * 1000) {
        throw new Error('State token expired');
      }
      
      return parsed;
    } catch (error) {
      throw new Error('Invalid state token');
    }
  }
}
```

### Figma OAuth Implementation

**File: `/src/lib/oauth/figma.ts`**
```typescript
import { BaseOAuthHandler, OAuthConfig } from './base';

export class FigmaOAuthHandler extends BaseOAuthHandler {
  constructor() {
    const config: OAuthConfig = {
      authorizationUrl: 'https://www.figma.com/oauth',
      tokenUrl: 'https://www.figma.com/api/oauth/token',
      clientId: process.env.FIGMA_CLIENT_ID!,
      clientSecret: process.env.FIGMA_CLIENT_SECRET!,
      scopes: ['file_read', 'file_write', 'file_comments'], // Adjust based on needs
      redirectUri: `${process.env.NEXT_PUBLIC_APP_URL}/api/oauth/figma/callback`,
    };
    
    super(config);
  }
  
  // Figma-specific: Get user info after authentication
  async getUserInfo(accessToken: string) {
    const response = await fetch('https://api.figma.com/v1/me', {
      headers: {
        'Authorization': `Bearer ${accessToken}`,
      },
    });
    
    if (!response.ok) {
      throw new Error('Failed to fetch Figma user info');
    }
    
    return response.json();
  }
}
```

### Integration-Specific Rate Limiting

**File: `/src/lib/rateLimit.ts`**
```typescript
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL!);

export async function checkRateLimit(
  integration: string,
  organizationId: string,
  cost: number = 1
): Promise<{ allowed: boolean; remaining: number; resetAt: Date }> {
  const key = `ratelimit:${integration}:${organizationId}`;
  
  // Get rate limit config for this integration
  const limits = {
    figma: { maxCredits: 6000, windowSeconds: 60 }, // 6000 credits/minute
    linear: { maxCredits: 1000, windowSeconds: 60 }, // 1000 req/minute
    github: { maxCredits: 5000, windowSeconds: 3600 }, // 5000 req/hour
    slack: { maxCredits: 50, windowSeconds: 60 }, // 50 req/minute (Tier 1)
    notion: { maxCredits: 180, windowSeconds: 60 }, // 3 req/second
    zoom: { maxCredits: 4800, windowSeconds: 60 }, // 80 req/second (Light APIs)
  };
  
  const limit = limits[integration as keyof typeof limits] || { maxCredits: 1000, windowSeconds: 60 };
  
  // Token bucket algorithm using Redis
  const now = Date.now();
  const windowStart = Math.floor(now / (limit.windowSeconds * 1000)) * (limit.windowSeconds * 1000);
  const windowKey = `${key}:${windowStart}`;
  
  // Get current usage
  const currentUsage = await redis.get(windowKey);
  const used = currentUsage ? parseInt(currentUsage) : 0;
  
  if (used + cost > limit.maxCredits) {
    // Rate limit exceeded
    const resetAt = new Date(windowStart + limit.windowSeconds * 1000);
    return {
      allowed: false,
      remaining: Math.max(0, limit.maxCredits - used),
      resetAt,
    };
  }
  
  // Increment usage
  const newUsage = await redis.incrby(windowKey, cost);
  
  // Set expiration if this is the first increment
  if (newUsage === cost) {
    await redis.expire(windowKey, limit.windowSeconds * 2); // 2x window for safety
  }
  
  const resetAt = new Date(windowStart + limit.windowSeconds * 1000);
  
  return {
    allowed: true,
    remaining: limit.maxCredits - newUsage,
    resetAt,
  };
}
```

---

## REAL-TIME COLLABORATION IMPLEMENTATION

### WebSocket Server Setup (Railway/DO)

**File: `/websocket-server/index.ts`**
```typescript
import express from 'express';
import { createServer } from 'http';
import { Server } from 'socket.io';
import Redis from 'ioredis';
import { createAdapter } from '@socket.io/redis-adapter';

const app = express();
const httpServer = createServer(app);

// Redis for pub/sub across multiple WebSocket servers
const pubClient = new Redis(process.env.REDIS_URL!);
const subClient = pubClient.duplicate();

const io = new Server(httpServer, {
  cors: {
    origin: process.env.NEXT_PUBLIC_APP_URL,
    credentials: true,
  },
  adapter: createAdapter(pubClient, subClient),
});

// Authentication middleware
io.use(async (socket, next) => {
  const token = socket.handshake.auth.token;
  
  if (!token) {
    return next(new Error('Authentication required'));
  }
  
  try {
    // Verify JWT token (Clerk or Supabase)
    const user = await verifyToken(token);
    socket.data.user = user;
    socket.data.organizationId = user.organizationId;
    next();
  } catch (error) {
    next(new Error('Invalid token'));
  }
});

// Connection handler
io.on('connection', (socket) => {
  const userId = socket.data.user.id;
  const organizationId = socket.data.organizationId;
  
  console.log(`User ${userId} connected`);
  
  // Join organization room (for org-wide broadcasts)
  socket.join(`org:${organizationId}`);
  
  // Handle thread subscription
  socket.on('thread:subscribe', (threadId: string) => {
    // Verify user has access to thread
    if (hasThreadAccess(userId, threadId)) {
      socket.join(`thread:${threadId}`);
      
      // Broadcast presence
      socket.to(`thread:${threadId}`).emit('presence:join', {
        userId,
        userName: socket.data.user.fullName,
        avatarUrl: socket.data.user.avatarUrl,
      });
    }
  });
  
  socket.on('thread:unsubscribe', (threadId: string) => {
    socket.leave(`thread:${threadId}`);
    socket.to(`thread:${threadId}`).emit('presence:leave', { userId });
  });
  
  // Handle collaborative editing (cursor positions)
  socket.on('cursor:move', (data: { threadId: string; x: number; y: number }) => {
    socket.to(`thread:${data.threadId}`).emit('cursor:update', {
      userId,
      x: data.x,
      y: data.y,
    });
  });
  
  // Handle typing indicators
  socket.on('typing:start', (data: { threadId: string }) => {
    socket.to(`thread:${data.threadId}`).emit('typing:update', {
      userId,
      isTyping: true,
    });
  });
  
  socket.on('typing:stop', (data: { threadId: string }) => {
    socket.to(`thread:${data.threadId}`).emit('typing:update', {
      userId,
      isTyping: false,
    });
  });
  
  // Handle disconnection
  socket.on('disconnect', () => {
    console.log(`User ${userId} disconnected`);
    // Broadcast presence leave to all threads user was in
    socket.rooms.forEach(room => {
      if (room.startsWith('thread:')) {
        io.to(room).emit('presence:leave', { userId });
      }
    });
  });
});

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'ok', connections: io.engine.clientsCount });
});

const PORT = process.env.PORT || 3001;
httpServer.listen(PORT, () => {
  console.log(`WebSocket server running on port ${PORT}`);
});
```

### Supabase Realtime Fallback

**File: `/src/lib/realtime/supabase.ts`**
```typescript
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

export function subscribeToThread(threadId: string, callbacks: {
  onItemAdded?: (item: any) => void;
  onCommentAdded?: (comment: any) => void;
  onThreadUpdated?: (thread: any) => void;
}) {
  // Subscribe to connected_items INSERT
  const itemsChannel = supabase
    .channel(`thread:${threadId}:items`)
    .on(
      'postgres_changes',
      {
        event: 'INSERT',
        schema: 'public',
        table: 'connected_items',
        filter: `thread_id=eq.${threadId}`,
      },
      (payload) => {
        callbacks.onItemAdded?.(payload.new);
      }
    )
    .subscribe();
  
  // Subscribe to comments INSERT
  const commentsChannel = supabase
    .channel(`thread:${threadId}:comments`)
    .on(
      'postgres_changes',
      {
        event: 'INSERT',
        schema: 'public',
        table: 'comments',
        filter: `thread_id=eq.${threadId}`,
      },
      (payload) => {
        callbacks.onCommentAdded?.(payload.new);
      }
    )
    .subscribe();
  
  // Subscribe to thread UPDATE
  const threadChannel = supabase
    .channel(`thread:${threadId}:updates`)
    .on(
      'postgres_changes',
      {
        event: 'UPDATE',
        schema: 'public',
        table: 'golden_threads',
        filter: `id=eq.${threadId}`,
      },
      (payload) => {
        callbacks.onThreadUpdated?.(payload.new);
      }
    )
    .subscribe();
  
  // Return cleanup function
  return () => {
    supabase.removeChannel(itemsChannel);
    supabase.removeChannel(commentsChannel);
    supabase.removeChannel(threadChannel);
  };
}
```

---

## SECURITY IMPLEMENTATION

### Encryption Utilities

**File: `/src/lib/encryption.ts`**
```typescript
import { createCipheriv, createDecipheriv, randomBytes, scryptSync } from 'crypto';

const ALGORITHM = 'aes-256-gcm';

// Get tenant-specific encryption key (derived from master key + tenant ID)
function getTenantKey(tenantId: string): Buffer {
  const masterKey = process.env.ENCRYPTION_MASTER_KEY!;
  return scryptSync(masterKey, tenantId, 32); // 32 bytes for AES-256
}

export async function encrypt(plaintext: string, tenantId: string): Promise<string> {
  const key = getTenantKey(tenantId);
  const iv = randomBytes(16); // Initialization vector
  const cipher = createCipheriv(ALGORITHM, key, iv);
  
  let encrypted = cipher.update(plaintext, 'utf8', 'hex');
  encrypted += cipher.final('hex');
  
  const authTag = cipher.getAuthTag();
  
  // Format: iv:authTag:encrypted
  return `${iv.toString('hex')}:${authTag.toString('hex')}:${encrypted}`;
}

export async function decrypt(ciphertext: string, tenantId: string): Promise<string> {
  const key = getTenantKey(tenantId);
  const parts = ciphertext.split(':');
  
  if (parts.length !== 3) {
    throw new Error('Invalid ciphertext format');
  }
  
  const iv = Buffer.from(parts[0], 'hex');
  const authTag = Buffer.from(parts[1], 'hex');
  const encrypted = parts[2];
  
  const decipher = createDecipheriv(ALGORITHM, key, iv);
  decipher.setAuthTag(authTag);
  
  let decrypted = decipher.update(encrypted, 'hex', 'utf8');
  decrypted += decipher.final('utf8');
  
  return decrypted;
}
```

### Row-Level Security (RLS) Setup

**File: `/prisma/migrations/001_enable_rls.sql`**
```sql
-- Enable RLS on all tenant-scoped tables
ALTER TABLE organizations ENABLE ROW LEVEL SECURITY;
ALTER TABLE users ENABLE ROW LEVEL SECURITY;
ALTER TABLE golden_threads ENABLE ROW LEVEL SECURITY;
ALTER TABLE connected_items ENABLE ROW LEVEL SECURITY;
ALTER TABLE comments ENABLE ROW LEVEL SECURITY;
ALTER TABLE integrations ENABLE ROW LEVEL SECURITY;
ALTER TABLE automations ENABLE ROW LEVEL SECURITY;

-- Create policy for organizations (users can only see their own org)
CREATE POLICY org_isolation ON organizations
  FOR ALL
  USING (id = current_setting('app.current_tenant')::UUID);

-- Create policy for users (see users in same org)
CREATE POLICY users_isolation ON users
  FOR ALL
  USING (organization_id = current_setting('app.current_tenant')::UUID);

-- Create policy for threads (see threads in same org, respecting visibility)
CREATE POLICY threads_isolation ON golden_threads
  FOR SELECT
  USING (
    organization_id = current_setting('app.current_tenant')::UUID
    AND (
      visibility = 'organization'
      OR created_by = current_setting('app.current_user')::UUID
      OR EXISTS (
        SELECT 1 FROM thread_collaborators
        WHERE thread_id = golden_threads.id
          AND user_id = current_setting('app.current_user')::UUID
      )
    )
  );

-- Similar policies for other tables...
```

### Middleware to Set RLS Context

**File: `/src/lib/middleware/rls.ts`**
```typescript
import { NextRequest, NextResponse } from 'next/server';
import { prisma } from '@/lib/prisma';

export async function withRLS(
  req: NextRequest,
  handler: (req: NextRequest) => Promise<NextResponse>
) {
  // Extract user session (Clerk or Supabase)
  const session = await getSession(req);
  
  if (!session) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }
  
  // Set RLS context variables
  await prisma.$executeRaw`
    SELECT set_config('app.current_tenant', ${session.organizationId}::text, true);
  `;
  
  await prisma.$executeRaw`
    SELECT set_config('app.current_user', ${session.userId}::text, true);
  `;
  
  // Execute handler with RLS context set
  return handler(req);
}
```

---

## AI & SEMANTIC SEARCH IMPLEMENTATION

### Embedding Generation Background Job

**File: `/src/inngest/functions/embeddings.ts`**
```typescript
import { inngest } from '@/lib/inngest';
import { prisma } from '@/lib/prisma';
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export const generateThreadEmbedding = inngest.createFunction(
  { id: 'generate-thread-embedding', retries: 3 },
  { event: 'thread.embedding.generate' },
  async ({ event }) => {
    const { threadId } = event.data;
    
    // Fetch thread
    const thread = await prisma.goldenThread.findUnique({
      where: { id: threadId },
      select: { title: true, description: true, tags: true },
    });
    
    if (!thread) {
      throw new Error(`Thread ${threadId} not found`);
    }
    
    // Construct text for embedding
    const text = [
      thread.title,
      thread.description,
      thread.tags.join(' '),
    ].filter(Boolean).join('\n');
    
    // Generate embedding using OpenAI
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text,
    });
    
    const embedding = response.data[0].embedding;
    
    // Store embedding
    await prisma.$executeRaw`
      UPDATE golden_threads
      SET embedding = ${JSON.stringify(embedding)}::vector
      WHERE id = ${threadId}
    `;
    
    return { success: true, threadId };
  }
);

export const generateItemEmbedding = inngest.createFunction(
  { id: 'generate-item-embedding', retries: 3 },
  { event: 'item.embedding.generate' },
  async ({ event }) => {
    const { itemId } = event.data;
    
    // Fetch item
    const item = await prisma.connectedItem.findUnique({
      where: { id: itemId },
      select: { title: true, description: true, integrationType: true },
    });
    
    if (!item) {
      throw new Error(`Item ${itemId} not found`);
    }
    
    // Construct text
    const text = [
      item.title,
      item.description,
      `Integration: ${item.integrationType}`,
    ].filter(Boolean).join('\n');
    
    // Generate embedding
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text,
    });
    
    const embedding = response.data[0].embedding;
    
    // Store embedding
    await prisma.$executeRaw`
      UPDATE connected_items
      SET embedding = ${JSON.stringify(embedding)}::vector
      WHERE id = ${itemId}
    `;
    
    return { success: true, itemId };
  }
);

// Batch embedding generation for cost efficiency
export const batchGenerateEmbeddings = inngest.createFunction(
  { id: 'batch-generate-embeddings' },
  { cron: '0 2 * * *' }, // Daily at 2 AM
  async () => {
    // Find threads without embeddings
    const threads = await prisma.goldenThread.findMany({
      where: { embedding: null, deletedAt: null },
      take: 1000,
      select: { id: true, title: true, description: true, tags: true },
    });
    
    if (threads.length === 0) {
      return { processed: 0 };
    }
    
    // Batch API call (up to 100 inputs per request)
    const batchSize = 100;
    for (let i = 0; i < threads.length; i += batchSize) {
      const batch = threads.slice(i, i + batchSize);
      
      const inputs = batch.map(thread => 
        [thread.title, thread.description, thread.tags.join(' ')].filter(Boolean).join('\n')
      );
      
      const response = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: inputs,
      });
      
      // Update all embeddings in batch
      for (let j = 0; j < batch.length; j++) {
        const thread = batch[j];
        const embedding = response.data[j].embedding;
        
        await prisma.$executeRaw`
          UPDATE golden_threads
          SET embedding = ${JSON.stringify(embedding)}::vector
          WHERE id = ${thread.id}
        `;
      }
    }
    
    return { processed: threads.length };
  }
);
```

### LLM Router for Cost Optimization

**File: `/src/lib/ai/router.ts`**
```typescript
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

export type ModelTier = 'fast' | 'smart';

export function classifyQuery(query: string): ModelTier {
  // Heuristics to determine query complexity
  const length = query.length;
  const hasQuestionMarks = (query.match(/\?/g) || []).length;
  const hasTechnicalTerms = /\b(architecture|implementation|optimize|analyze|debug)\b/i.test(query);
  
  // Simple queries → fast model (GPT-3.5)
  if (length < 100 && hasQuestionMarks <= 1 && !hasTechnicalTerms) {
    return 'fast';
  }
  
  // Complex queries → smart model (Claude Sonnet)
  return 'smart';
}

export async function generateCompletion(
  prompt: string,
  context?: string,
  tier?: ModelTier
): Promise<string> {
  const selectedTier = tier || classifyQuery(prompt);
  
  if (selectedTier === 'fast') {
    // Use GPT-3.5-turbo for simple queries
    const response = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content: 'You are a helpful assistant for product teams using Synapse.',
        },
        ...(context ? [{ role: 'system', content: `Context: ${context}` }] : []),
        { role: 'user', content: prompt },
      ],
      temperature: 0.7,
      max_tokens: 500,
    });
    
    return response.choices[0].message.content || '';
  } else {
    // Use Claude Sonnet for complex queries with prompt caching
    const response = await anthropic.messages.create({
      model: 'claude-sonnet-4-20250514',
      max_tokens: 1024,
      system: [
        {
          type: 'text',
          text: 'You are a helpful assistant for product teams using Synapse, an integration platform connecting design and development tools.',
          cache_control: { type: 'ephemeral' }, // Cache system prompt
        },
        ...(context ? [{
          type: 'text',
          text: `Context from user's project:\n${context}`,
          cache_control: { type: 'ephemeral' }, // Cache context
        }] : []),
      ],
      messages: [
        { role: 'user', content: prompt },
      ],
    });
    
    return response.content[0].type === 'text' ? response.content[0].text : '';
  }
}
```

---

## DEPLOYMENT CONFIGURATION

### Railway Setup (Recommended)

**File: `railway.toml`**
```toml
[build]
builder = "NIXPACKS"

[deploy]
startCommand = "npm run start"
healthcheckPath = "/api/health"
healthcheckTimeout = 10
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 3

[[services]]
name = "web"
plan = "hobby" # Starts at $5/month
replicas = 1
autoscaling = true
minReplicas = 1
maxReplicas = 3

[[services]]
name = "websocket"
plan = "hobby"
replicas = 1
startCommand = "npm run websocket"
```

### Vercel Configuration

**File: `vercel.json`**
```json
{
  "buildCommand": "prisma generate && next build",
  "devCommand": "next dev",
  "framework": "nextjs",
  "installCommand": "npm install",
  "regions": ["iad1", "sfo1"],
  "env": {
    "DATABASE_URL": "@database_url",
    "NEXT_PUBLIC_APP_URL": "@app_url",
    "CLERK_SECRET_KEY": "@clerk_secret",
    "OPENAI_API_KEY": "@openai_key",
    "ANTHROPIC_API_KEY": "@anthropic_key"
  },
  "crons": [
    {
      "path": "/api/cron/embeddings",
      "schedule": "0 2 * * *"
    },
    {
      "path": "/api/cron/cleanup",
      "schedule": "0 3 * * *"
    }
  ]
}
```

### Environment Variables Checklist

```bash
# Database
DATABASE_URL="postgresql://..."
DATABASE_DIRECT_URL="postgresql://..." # For migrations

# Auth
CLERK_SECRET_KEY="sk_live_..."
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY="pk_live_..."
# OR
SUPABASE_URL="https://xxx.supabase.co"
SUPABASE_ANON_KEY="eyJ..."
SUPABASE_SERVICE_ROLE_KEY="eyJ..."

# Storage
R2_ACCOUNT_ID="..."
R2_ACCESS_KEY_ID="..."
R2_SECRET_ACCESS_KEY="..."
R2_BUCKET_NAME="synapse-prod"

# Redis
REDIS_URL="redis://..."

# AI
OPENAI_API_KEY="sk-..."
ANTHROPIC_API_KEY="sk-ant-..."

# Integrations (OAuth)
FIGMA_CLIENT_ID="..."
FIGMA_CLIENT_SECRET="..."
LINEAR_CLIENT_ID="..."
LINEAR_CLIENT_SECRET="..."
GITHUB_APP_ID="..."
GITHUB_PRIVATE_KEY="..."
SLACK_CLIENT_ID="..."
SLACK_CLIENT_SECRET="..."
NOTION_CLIENT_ID="..."
NOTION_CLIENT_SECRET="..."
ZOOM_CLIENT_ID="..."
ZOOM_CLIENT_SECRET="..."

# App Config
NEXT_PUBLIC_APP_URL="https://app.synapse.com"
ENCRYPTION_MASTER_KEY="..." # Generate with: openssl rand -base64 32
NODE_ENV="production"
```

---

## PERFORMANCE OPTIMIZATION

### Caching Strategy

```typescript
// File: /src/lib/cache.ts

import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL!);

export async function cacheGet<T>(key: string): Promise<T | null> {
  const cached = await redis.get(key);
  return cached ? JSON.parse(cached) : null;
}

export async function cacheSet(
  key: string,
  value: any,
  ttlSeconds: number = 300
): Promise<void> {
  await redis.setex(key, ttlSeconds, JSON.stringify(value));
}

export async function cacheDelete(key: string): Promise<void> {
  await redis.del(key);
}

// Invalidation patterns
export async function invalidateThreadCache(threadId: string): Promise<void> {
  const patterns = [
    `thread:${threadId}:*`,
    `threads:list:*`, // Invalidate all thread lists
  ];
  
  for (const pattern of patterns) {
    const keys = await redis.keys(pattern);
    if (keys.length > 0) {
      await redis.del(...keys);
    }
  }
}
```

### Database Query Optimization

```sql
-- Critical indexes for performance
CREATE INDEX CONCURRENTLY idx_threads_org_status_activity 
  ON golden_threads(organization_id, status, last_activity_at DESC);

CREATE INDEX CONCURRENTLY idx_items_thread_created 
  ON connected_items(thread_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_comments_thread_created 
  ON comments(thread_id, created_at DESC) WHERE is_deleted = false;

-- Composite index for search + filters
CREATE INDEX CONCURRENTLY idx_items_org_integration_search 
  ON connected_items(organization_id, integration_type) 
  INCLUDE (title, description);
```

---

## MONITORING & OBSERVABILITY

### Logging Strategy

```typescript
// File: /src/lib/logger.ts

import winston from 'winston';
import { Axiom } from '@axiomhq/js';

const axiom = new Axiom({
  token: process.env.AXIOM_TOKEN!,
  orgId: process.env.AXIOM_ORG_ID!,
});

export const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.json(),
  defaultMeta: { service: 'synapse-api' },
  transports: [
    new winston.transports.Console({
      format: winston.format.simple(),
    }),
    new AxiomTransport(),
  ],
});

class AxiomTransport extends winston.Transport {
  async log(info: any, callback: () => void) {
    setImmediate(() => this.emit('logged', info));
    
    await axiom.ingest('synapse-logs', [
      {
        _time: new Date().toISOString(),
        level: info.level,
        message: info.message,
        ...info,
      },
    ]);
    
    callback();
  }
}
```

---

## CONCLUSION & IMPLEMENTATION PRIORITIES

### Week-by-Week Development Plan

**Weeks 1-2: Foundation**
- Infrastructure setup (Railway, Supabase, Cloudflare R2)
- Database schema migration
- Authentication (Clerk integration)
- Basic Next.js app structure with tRPC

**Weeks 3-4: Core Features**
- Golden Thread CRUD operations
- Dashboard UI with thread cards
- Thread detail view with timeline
- Real-time updates (Supabase Realtime)

**Weeks 5-6: Integrations (Tier 1)**
- GitHub OAuth + webhook handler
- Slack OAuth + webhook handler
- Linear OAuth + webhook handler
- Connected items sync

**Weeks 7-8: Integrations (Tier 2) + AI**
- Figma OAuth + polling
- Notion OAuth + polling
- Semantic search (embeddings + pgvector)
- AI insights generation

**Weeks 9-10: Integrations (Tier 3) + Automation**
- Zoom, Dovetail, Mixpanel integrations
- Visual automation builder UI
- Automation execution engine (Inngest)
- Pre-built templates

**Weeks 11-12: Polish & Launch**
- Security hardening (RLS, encryption, rate limiting)
- Performance optimization (caching, indexes)
- Onboarding flow + documentation
- Billing integration (Stripe)
- Production deployment + monitoring

### Critical Success Factors

1. **Start with free tiers everywhere** - Vercel, Supabase, Cloudflare, Inngest all free initially
2. **Implement Tier 1 integrations first** - GitHub, Slack, Linear are easiest and most valuable
3. **Use tRPC for type safety** - Catch bugs at compile time, not runtime
4. **Cache aggressively** - Redis caching saves database load and AI costs
5. **Monitor from day 1** - Axiom, Better Stack, Rollbar free tiers provide essential visibility

### Budget Allocation (Month 1-3)

```
Infrastructure:        $80-180/month
AI (embeddings + LLM): $50-150/month
Auth (Clerk → Supabase): $0-25/month
Monitoring:            $0/month (free tiers)
Email (Resend):        $0/month (free tier)
Total:                 $130-355/month
```

**Budget remaining for:** Overages, experiments, paid marketing

This technical specification provides Claude Code with everything needed to implement Synapse as a production-grade platform within the $500/month budget constraint. Focus on using free tiers strategically, optimizing AI costs through smart routing, and leveraging modern infrastructure platforms that align costs with usage.
